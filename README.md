# ML-Adversarial-Attacks-and-Defenses

This project explores the security vulnerabilities of machine learning models, focusing on data poisoning and adversarial attacks. It also implements various defense mechanisms to enhance model robustness against adversarial examples. 

## Overview
In this project:
1. Build a Convolutional Neural Network (CNN) model to classify images from the MNIST dataset.
2. Generate adversarial examples using techniques such as Fast Gradient Sign Method (FGSM), Projected Gradient Descent (PGD), and DeepFool.
3. Evaluate the vulnerability of the model to these adversarial attacks.
4. Implement data poisoning by injecting adversarial examples into the training set.
5. Apply defense mechanisms, including adversarial training, to improve model robustness.
6. Measure and compare the model's performance before and after implementing defenses.




